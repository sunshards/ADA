from openai import OpenAI
from dotenv import load_dotenv
import os
import time
import re
import json

# Global variables for long-term memory management 
system_rules =  {
            "role": "system",
            "content": """You are the master of a D&D-style fantasy text adventure.
                            You must ALWAYS respond in valid JSON, with this structure:

        {
        "narration": "descrizione immersiva della scena",
        "found_items": [],
        "lost_items": [],
        "location": "current location of the player",
        "quest": "current quest of the player",
        }

        Regole:
        - narration: narrative text only
        - found_items: items found by the player
        - lost_items: lost items
        - Never decide for the player
        - Output ONLY raw JSON
        - Do NOT use markdown
        - Do NOT add explanations
        - Do NOT wrap the JSON in ``` fences
        - Output must start with { and end with }
        """
}
turn_count = 0
recent_history = []
#! Make sure to change the strat point, so you can get the places from the approved database
long_term_memory = "Il personaggio si trova nella Taverna Iniziale. Nessun evento rilevante finora."


# Load the .env file -> so it takes the api key (remember to create it)
load_dotenv()

# Client OpenRouter
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)

# Fall back to call if another free model is not available
FREE_MODELS = [
    "mistralai/devstral-2512:free", # Damn Frog Eater from the other side of the Apls
    "nex-agi/deepseek-v3.1-nex-n1:free",
    "openai/gpt-oss-120b:free",
    "google/gemma-3n-e2b-it:free",
    "meta-llama/llama-guard-4-12b:free",
    "openrouter/auto"
]


# Shoud I use """ or # for the documentation?

# 3 tries for models distanced by a 2 second delay each one then fallback to the next one
def narrate(history, retries=3, delay=2):
    """
    Generate the narrative by trying the free models in order.
    If a model fails, try the next one.
    retries: total number of 3 attempts per model in case of rate limiting
    delay: 2 seconds to wait before retrying
    """

    for model in FREE_MODELS:
        for attempt in range(retries):
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=history,
                    max_tokens=400
                )
                return response.choices[0].message.content
            except Exception as e:
                print(f"[WARN] Model {model} failed attempt {attempt+1}: {e}")
                time.sleep(delay)
        print(f"[INFO] Passing to next model...")

    return "The narrator is temporarily out of voice. Please try again shortly."



# --- Test CLI ---
# if __name__ == "__main__":
#     print("MAIN RUNNING")

#     result = narrate(
#         character={"nome": "Arin", "classe": "Guerriero"},
#         state={"Plocation": "Taverna Iniziale", "quest": "Nessuna"},
#         user_input="Descrivi una taverna fantasy"
#     )

#     print("RESULT:")
#     print(result)


# We cannot trust the model to always return valid JSON, so we need to extract it from the text
def extract_json(text):
    # I stole this, don't judge me. Regrex are black magic to me! :-(
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if not match:
        raise ValueError("No found JASON object in the text")
    return json.loads(match.group())



# ! Dice roll for the chance to encounter an enemy (cr4 and 2cr3 enemies like dnd) when entering a dungeon
# ! The dungeon is generated by an algorithm I will stole from GitHub


# TODO: Ensure that the state is saved. Ensure that the save is the 
# TODO: result of a prompt "Summarise the most important events" (Exampre: Claude playing PokÃ©mon)
# TODO: implentat a function that count the max token used by the history and truncate old messages if needed 
# from point to maxthokens (point2) -> summary1 
# from point2 to maxthokens (point3) -> summary2
# Context saved: summary1 + summary2 + recent messages (?)
def summarise_memory(long_memory, recent_history):
    messages = [
        {
            "role": "system",
            "content": "You summarise a D&D adventure for long-term memory."
        },
        {
            "role": "user",
            "content": f"""
        Current memory: {long_memory}
        Recent events: {recent_history}

        Update the memory keeping ONLY important facts:
        - locations
        - quests
        - items
        - major decisions
        """
        }
    ]

    # Call the model to get the summary
    summary = narrate(messages)

    return summary



# --- Loop di gioco CLI ---
def main():
    # Python has to explicitly state that we are using the global variables (the serpet is cleraly not fit to be a C competitor :-P)
    global long_term_memory
    global turn_count
    global recent_history
    print("=== ADA TI DA' IL BENVENUTO ===")

    # Initial Character Sheet and State
    # TODO: Load the character and state from a dataabse save file if exists
    character = {"salute": 100,
                 "Name": "Monty", 
                 "Razza": "Toro Umano", 
                 "Classe": "Cs Graduate",  
                 "inventory": ["spada corta"]}

    state = {"Plocation": "Taverna Iniziale", 
             "Pquest": "Nessuna"}


    history = [
        system_rules,
        {"role": "system", "content": f"Long-term memory: {long_term_memory}"},
        {"role": "system", "content": f"Character: {character}"},
        {"role": "system", "content": f"State: {state}"},
    ] + recent_history[-8:]  # last 8 messages (4 ai + 4 user = 4 completed turns) for context



    while True:
        if turn_count > 0 and turn_count % 10 == 0:
            long_term_memory = summarise_memory(long_term_memory, recent_history)
            recent_history = []

        user_input = input("\nCosa fai? ")
        turn_count += 1
        if user_input.lower() in ["exit", "quit", "esci"]:
            break
        
        # Add current user response to history (so the model remembers your actions)
        recent_history.append({"role": "user", "content": user_input})

        # Call the model to get the narrative response
        output = narrate(history)

        try:
            data = extract_json(output)
        except json.JSONDecodeError:
            print("Narrator Confused! JASON Inccorecty generated:")
            print(output)
            return

        # Add/remove items from inventory
        for item in data["found_items"]:
            if item not in character["inventory"]:
                character["inventory"].append(item)

        for item in data["lost_items"]:
            if item in character["inventory"]:
                character["inventory"].remove(item)

        # Update the location and current quest (P-refix for prompt variables)
        if "location" in data:
            state["Plocation"] = data["location"]
        if "quest" in data:
            state["Pquest"] = data["quest"]

        # Add the response to history (so the context is preserved)
        recent_history.append({"role": "assistant", "content": output})
        history.append({
            "role": "system",
            "content": f"Updated character sheet: {character}"
        })
        history.append({
            "role": "system",
            "content": f"Adventure status updated: {state}"
        })

        # We give the player only the narration part
        print("\n" + data["narration"])

if __name__ == "__main__":
    main()
